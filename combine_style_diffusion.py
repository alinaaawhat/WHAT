import torch
import os
import numpy as np
from datetime import datetime
import argparse
import sys
sys.path.append('.')
sys.path.append('./Style_conditioner/')
sys.path.append('./Diffusion_model/')

# Style conditioner imports
from Style_conditioner.utils import _logger, set_requires_grad
from Style_conditioner.trainer.trainer import Trainer, Trainer_ft
from Style_conditioner.models.TC import TC
from Style_conditioner.models.model import base_Model

# Diffusion model imports
from Diffusion_model.denoising_diffusion_pytorch import Trainer1D_Train as Trainer1D, Unet1D_cond_train as Unet1D_cond, GaussianDiffusion1Dcond_train as GaussianDiffusion1Dcond

# Data loading
from data_load.get_domainhar import get_acthar,get_acthar_client,get_acthar_client_fre
import torch.utils.data as data

# Set device
device = 'cuda' if torch.cuda.is_available() else 'cpu'

def train_style_conditioner_client(args, client_id, global_model_path=None):
    """ä¸ºæŒ‡å®šclientè®­ç»ƒStyle Conditioner"""
    print("=" * 50)
    print(f"å¼€å§‹ä¸ºClient {client_id}è®­ç»ƒ Style Conditioner")
    print("=" * 50)
    
    start_time = datetime.now()
    
    # Some key info
    data_type = args.dataset
    target = args.target
    remain_rate = args.remain_rate
    SEED = args.seed
    testuser = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_client{client_id}'
    batch_size = args.batch_size
    
    # Load data for specific client
    train_loader, valid_loader, target_loader, _ = get_acthar_client_fre(args, data_type, target, batch_size=args.batch_size, remain_rate=remain_rate, seed=SEED, train_diff=0, client_id=client_id)
    # for batch in train_loader:
    #     data, labels = batch[0], batch[1] 
    #     print(f"æ•°æ®å½¢çŠ¶: {data.shape}, æ•°æ®èŒƒå›´: [{data.min():.4f}, {data.max():.4f}]")
    #     if torch.isnan(data).any():
    #         print("âŒ æ•°æ®åŒ…å«NaNï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
    #     break
    train_dataset = train_loader.dataset
    source_loaders = data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)

    # Import config dynamicallynow
    import importlib
    module_name = f'Style_conditioner.config_files.{data_type}_Configs'
    ConfigModule = importlib.import_module(module_name)
    configs = ConfigModule.Config()
    configs.batch_size = batch_size

    # Fix random seeds for reproducibility
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = False
    np.random.seed(SEED)

    # Setup logging
    experiment_log_dir = os.path.join(args.logs_save_dir, data_type+str(remain_rate)+f"_seed_{SEED}")
    os.makedirs(experiment_log_dir, exist_ok=True)
    log_file_name = os.path.join(experiment_log_dir, f"logs_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}.log")
    logger = _logger(log_file_name)
    logger.debug("=" * 45)
    logger.debug(f'Dataset: {data_type}')
    logger.debug(f'Mode:    {args.training_mode}')
    logger.debug("=" * 45)

    # Load Model
    model = base_Model(configs).to(device)
    temporal_contr_model = TC(configs, device).to(device)
    
    # Load global model if provided (for federated learning rounds after the first)
    if global_model_path and os.path.exists(global_model_path):
        print(f"Loading global model from: {global_model_path}")
        global_checkpoint = torch.load(global_model_path, map_location=device)
        model.load_state_dict(global_checkpoint['model_state_dict'])
        temporal_contr_model.load_state_dict(global_checkpoint['temporal_contr_state_dict'])

    if args.training_mode == "fine_tune":
        load_from = experiment_log_dir
        chkpoint_path = './Style_conditioner/conditioner_pth/global_style_conditioner/uschad_tar_0_rm_0.2seed_1_global.pt'
        chkpoint = torch.load(chkpoint_path, map_location=device)
        logs_save_dir = './Style_conditioner/conditioner_pth/'
        experiment_log_dir = os.path.join(logs_save_dir, data_type+str(remain_rate)+f"_seed_{SEED}")
        pretrained_dict = chkpoint["model_state_dict"]
        model_dict = model.state_dict()
        del_list = ['logits']
        pretrained_dict_copy = pretrained_dict.copy()
        for i in pretrained_dict_copy.keys():
            for j in del_list:
                if j in i:
                    del pretrained_dict[i]
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)
        # åœ¨åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨ä¹‹é—´æ·»åŠ ï¼š
        print("ğŸ”§ ä¿®å¤æ•°æ®ç±»å‹...")

        # ä¿®å¤train_loaderçš„æ•°æ®ç±»å‹
        original_train_loader = train_loader
        train_dataset = train_loader.dataset

        model_optimizer = torch.optim.Adam(model.parameters(), lr=configs.lr, betas=(configs.beta1, configs.beta2), weight_decay=3e-4)
        temporal_contr_optimizer = torch.optim.Adam(temporal_contr_model.parameters(), lr=configs.lr, betas=(configs.beta1, configs.beta2), weight_decay=3e-4)
        #########################
        # model_optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)
        # temporal_contr_optimizer = torch.optim.Adam(temporal_contr_model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)
        logger.debug(f"Training time is : {datetime.now()-start_time}")
        Trainer_ft(model, temporal_contr_model, model_optimizer, temporal_contr_optimizer, source_loaders, valid_loader, target_loader, device, logger, configs, experiment_log_dir, args.training_mode, testuser)
    else:
        model_optimizer = torch.optim.Adam(model.parameters(), lr=configs.lr, betas=(configs.beta1, configs.beta2), weight_decay=3e-4)
        temporal_contr_optimizer = torch.optim.Adam(temporal_contr_model.parameters(), lr=configs.lr, betas=(configs.beta1, configs.beta2), weight_decay=3e-4)
        Trainer(model, temporal_contr_model, model_optimizer, temporal_contr_optimizer, source_loaders, valid_loader, target_loader, device, logger, configs, experiment_log_dir, args.training_mode, testuser)
        logger.debug(f"Training time is : {datetime.now()-start_time}")
    
    print(f"Client {client_id} Style Conditioner è®­ç»ƒå®Œæˆ!")
    return testuser, model, temporal_contr_model
# def train_style_conditioner_client(args, client_id, global_model_path=None):
#     """ä¸ºæŒ‡å®šclientè®­ç»ƒStyle Conditioner"""
#     print("=" * 50)
#     print(f"å¼€å§‹ä¸ºClient {client_id}è®­ç»ƒ Style Conditioner")
#     print("=" * 50)
    
#     start_time = datetime.now()
    
#     # Some key info
#     data_type = args.dataset
#     target = args.target
#     remain_rate = args.remain_rate
#     SEED = args.seed
#     testuser = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_client{client_id}'
#     batch_size = args.batch_size
    
#     # Load data for specific client
#     train_loader, valid_loader, target_loader, _ = get_acthar_client_fre(args, data_type, target, batch_size=args.batch_size, remain_rate=remain_rate, seed=SEED, train_diff=0, client_id=client_id)
    
#     # ğŸ”§ æ•°æ®æ£€æŸ¥å’Œä¿®å¤ï¼ˆç§»åˆ°è¿™é‡Œï¼Œæ‰€æœ‰æ¨¡å¼éƒ½æ‰§è¡Œï¼‰
#     print("ğŸ” æ£€æŸ¥æ•°æ®...")
#     for batch in train_loader:
#         data, labels = batch[0], batch[1] 
#         print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
#         print(f"æ•°æ®ç±»å‹: {data.dtype}")
#         print(f"æ ‡ç­¾ç±»å‹: {labels.dtype}")
        
#         # æ£€æŸ¥æ•°æ®èŒƒå›´ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
#         if data.dtype not in [torch.complex128, torch.complex64]:
#             print(f"æ•°æ®èŒƒå›´: [{data.min():.4f}, {data.max():.4f}]")
#         else:
#             print("âš ï¸ æ£€æµ‹åˆ°å¤æ•°æ•°æ®ç±»å‹")
        
#         if torch.isnan(data).any():
#             print("âŒ æ•°æ®åŒ…å«NaNï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
#         break
    
#     # ğŸ”§ å®šä¹‰æ•°æ®ç±»å‹ä¿®å¤å‡½æ•°
#     def collate_fn_fixed(batch):
#         data_list, label_list = [], []
#         for data, label in batch:
#             # ä¿®å¤æ•°æ®ç±»å‹
#             if data.dtype in [torch.complex128, torch.complex64]:
#                 data = data.real.float()  # å–å®éƒ¨
#             else:
#                 data = data.float()  # ç¡®ä¿æ˜¯float32
#             label = label.long()  # ç¡®ä¿æ ‡ç­¾æ˜¯long
#             data_list.append(data)
#             label_list.append(label)
#         return torch.stack(data_list), torch.stack(label_list)
    
#     # ğŸ”§ é‡æ–°åˆ›å»ºä¿®å¤åçš„æ•°æ®åŠ è½½å™¨
#     train_dataset = train_loader.dataset
#     train_loader = torch.utils.data.DataLoader(
#         train_dataset, 
#         batch_size=batch_size, 
#         drop_last=True, 
#         shuffle=True,
#         collate_fn=collate_fn_fixed
#     )
    
#     valid_dataset = valid_loader.dataset  
#     valid_loader = torch.utils.data.DataLoader(
#         valid_dataset,
#         batch_size=batch_size,
#         drop_last=False,
#         shuffle=False,
#         collate_fn=collate_fn_fixed
#     )
    
#     target_dataset = target_loader.dataset
#     target_loader = torch.utils.data.DataLoader(
#         target_dataset,
#         batch_size=batch_size, 
#         drop_last=False,
#         shuffle=False,
#         collate_fn=collate_fn_fixed
#     )
    
#     source_loaders = train_loader

#     # Import config
#     import importlib
#     module_name = f'Style_conditioner.config_files.{data_type}_Configs'
#     ConfigModule = importlib.import_module(module_name)
#     configs = ConfigModule.Config()
#     configs.batch_size = batch_size

#     # Fix random seeds
#     torch.manual_seed(SEED)
#     torch.backends.cudnn.deterministic = False
#     torch.backends.cudnn.benchmark = False
#     np.random.seed(SEED)

#     # Setup logging
#     experiment_log_dir = os.path.join(args.logs_save_dir, data_type+str(remain_rate)+f"_seed_{SEED}")
#     os.makedirs(experiment_log_dir, exist_ok=True)
#     log_file_name = os.path.join(experiment_log_dir, f"logs_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}.log")
#     logger = _logger(log_file_name)
#     logger.debug("=" * 45)
#     logger.debug(f'Dataset: {data_type}')
#     logger.debug(f'Mode:    {args.training_mode}')
#     logger.debug("=" * 45)

#     # Load Model
#     model = base_Model(configs).to(device)
#     temporal_contr_model = TC(configs, device).to(device)
    
#     # Load global model if provided
#     if global_model_path and os.path.exists(global_model_path):
#         print(f"Loading global model from: {global_model_path}")
#         global_checkpoint = torch.load(global_model_path, map_location=device)
#         model.load_state_dict(global_checkpoint['model_state_dict'])
#         temporal_contr_model.load_state_dict(global_checkpoint['temporal_contr_state_dict'])

#     # ğŸ”§ ç»Ÿä¸€çš„ä¼˜åŒ–å™¨è®¾ç½®ï¼ˆä¿®å¤å­¦ä¹ ç‡å’Œå‚æ•°ï¼‰
#     model_optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)
#     temporal_contr_optimizer = torch.optim.Adam(temporal_contr_model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)

#     if args.training_mode == "fine_tune":
#         load_from = experiment_log_dir
#         chkpoint_path = './Style_conditioner/conditioner_pth/global_style_conditioner/uschad_tar_0_rm_0.2seed_1_global.pt'
#         chkpoint = torch.load(chkpoint_path, map_location=device)
#         logs_save_dir = './Style_conditioner/conditioner_pth/'
#         experiment_log_dir = os.path.join(logs_save_dir, data_type+str(remain_rate)+f"_seed_{SEED}")
#         pretrained_dict = chkpoint["model_state_dict"]
#         model_dict = model.state_dict()
#         del_list = ['logits']
#         pretrained_dict_copy = pretrained_dict.copy()
#         for i in pretrained_dict_copy.keys():
#             for j in del_list:
#                 if j in i:
#                     del pretrained_dict[i]
#         model_dict.update(pretrained_dict)
#         model.load_state_dict(model_dict)
        
#         logger.debug(f"Training time is : {datetime.now()-start_time}")
#         Trainer_ft(model, temporal_contr_model, model_optimizer, temporal_contr_optimizer, source_loaders, valid_loader, target_loader, device, logger, configs, experiment_log_dir, args.training_mode, testuser)
#     else:
#         Trainer(model, temporal_contr_model, model_optimizer, temporal_contr_optimizer, source_loaders, valid_loader, target_loader, device, logger, configs, experiment_log_dir, args.training_mode, testuser)
#         logger.debug(f"Training time is : {datetime.now()-start_time}")
    
#     print(f"Client {client_id} Style Conditioner è®­ç»ƒå®Œæˆ!")
#     return testuser, model, temporal_contr_model
def aggregate_style_conditioners(client_models, client_temporal_models, args, round_num):
    """èšåˆå„ä¸ªclientçš„Style Conditioner"""
    print("=" * 50)
    print("å¼€å§‹èšåˆå„Clientçš„Style Conditioners")
    print("=" * 50)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªclientçš„æ¨¡å‹ä½œä¸ºåŸºç¡€
    global_model = client_models[0]
    global_temporal_model = client_temporal_models[0]
    
    # è·å–çŠ¶æ€å­—å…¸
    global_state_dict = global_model.state_dict()
    global_temporal_state_dict = global_temporal_model.state_dict()
    
    # æ”¶é›†æ‰€æœ‰clientçš„å‚æ•°
    client_state_dicts = [model.state_dict() for model in client_models]
    client_temporal_state_dicts = [model.state_dict() for model in client_temporal_models]
    
    # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œå¹³å‡ - base_Model
    for key in global_state_dict.keys():
        param_sum = torch.zeros_like(global_state_dict[key])
        for client_state_dict in client_state_dicts:
            param_sum += client_state_dict[key]
        global_state_dict[key] = param_sum / len(client_models)
    
    # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œå¹³å‡ - TC (temporal contrastive model)
    for key in global_temporal_state_dict.keys():
        param_sum = torch.zeros_like(global_temporal_state_dict[key])
        for client_temporal_state_dict in client_temporal_state_dicts:
            param_sum += client_temporal_state_dict[key]
        global_temporal_state_dict[key] = param_sum / len(client_temporal_models)
    
    # æ›´æ–°å…¨å±€æ¨¡å‹
    global_model.load_state_dict(global_state_dict)
    global_temporal_model.load_state_dict(global_temporal_state_dict)
    
    # ä¿å­˜èšåˆåçš„Style Conditioner
    global_style_folder = os.path.join(args.logs_save_dir, 'global_style_conditioner')
    os.makedirs(global_style_folder, exist_ok=True)
    
    data_type = args.dataset
    target = args.target
    remain_rate = args.remain_rate
    SEED = args.seed
    global_style_name = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_global'
    
    # ä¿å­˜å…¨å±€Style Conditioner
    global_style_path = os.path.join(global_style_folder, global_style_name + '.pt')
    torch.save({
        'model_state_dict': global_model.state_dict(),
        'temporal_contr_state_dict': global_temporal_model.state_dict(),
        'round': round_num
    }, global_style_path)
    
    print(f"å…¨å±€Style Conditionerå·²ä¿å­˜åˆ°: {global_style_path}")
    print("Style Conditionerèšåˆå®Œæˆ!")
    return global_model, global_temporal_model, global_style_path

def train_diffusion_model_client(args, testuser, client_id, global_diffusion_model=None):
    """ä¸ºæŒ‡å®šclientè®­ç»ƒDiffusion Model"""
    print("=" * 50)
    print(f"å¼€å§‹ä¸ºClient {client_id}è®­ç»ƒ Diffusion Model")
    print("=" * 50)
    
    # Prepare some key info 
    data_type = args.dataset
    target = args.target
    remain_rate = args.remain_rate
    
    testuser_dict = {}
    testuser_dict['seed'] = args.seed
    testuser_dict['name'] = testuser
    global_style_folder = os.path.join(args.logs_save_dir, 'global_style_conditioner')
    os.makedirs(global_style_folder, exist_ok=True)

    SEED = args.seed
    global_style_name = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_global'
    
    
    global_style_path = os.path.join(global_style_folder, global_style_name + '.pt')
    print(global_style_path)

    # æ·»åŠ è°ƒè¯•å’Œæ£€æŸ¥
    print(f"ğŸ” Debug - Client {client_id} æœŸæœ›çš„Style Conditionerè·¯å¾„: {global_style_path}")
    print(f"ğŸ” Debug - æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(global_style_path)}")
    
    # if os.path.exists(global_style_path):
    #     # æ£€æŸ¥æ–‡ä»¶å†…å®¹
    #     import torch
    #     try:
    #         ckpt = torch.load(global_style_path, map_location='cpu')
    #         model_dict = ckpt['model_state_dict']
    #         conv1_shape = model_dict['conv_block1.0.weight'].shape
    #         logits_shape = model_dict['logits.weight'].shape
    #         print(f"ğŸ” Debug - æ–‡ä»¶ä¸­çš„æ¨¡å‹é…ç½®:")
    #         print(f"    conv1é€šé“æ•°: {conv1_shape[1]}")
    #         print(f"    è¾“å‡ºç±»åˆ«æ•°: {logits_shape[0]}")
            
    #         # æ£€æŸ¥æ˜¯å¦åŒ¹é…å½“å‰æ•°æ®é›†
    #         if conv1_shape[1] != 6 or logits_shape[0] != 12:
    #             print(f"âš ï¸  è­¦å‘Š: Style Conditioneré…ç½®ä¸USC-HADä¸åŒ¹é…!")
    #             print(f"    æœŸæœ›: 6é€šé“, 12ç±»åˆ«")
    #             print(f"    å®é™…: {conv1_shape[1]}é€šé“, {logits_shape[0]}ç±»åˆ«")
    #     except Exception as e:
    #         print(f"âŒ è¯»å–Style Conditioneræ–‡ä»¶å¤±è´¥: {e}")
    # else:
    #     print(f"âŒ Style Conditioneræ–‡ä»¶ä¸å­˜åœ¨!")
    
    testuser_dict['conditioner'] = global_style_path
    # conditioner = os.getcwd()+os.path.join('/Style_conditioner/conditioner_pth/')
    testuser_dict['conditioner'] = global_style_path
    # testuser_dict['conditioner'] = '/home/SHIH0020/robustlearn/Style_conditioner/conditioner_pth/global_style_conditioner/uschad_tar_0_rm_0.2seed_1_global.pt'
    train_loader, valid_loader, target_loader, testuser_dict['n_class'] = get_acthar_client_fre(args, data_type, target, batch_size=64, remain_rate=remain_rate, seed=testuser_dict['seed'], client_id=client_id)
    source_loaders = train_loader
    
    # Remaining data
    testuser_dict['remain_data'] = remain_rate
    print(f"Client {client_id} Remain:", testuser_dict['remain_data'])

    for minibatch in source_loaders:
        batch_size = minibatch[0].shape[0]
        print(f"Client {client_id} print shape X:", minibatch[0].shape)
        shapex = [minibatch[0].shape[0], minibatch[0].shape[1], minibatch[0].shape[2]]  # length,channel
        break
    
    if shapex[1] % 64 != 0:  # Pad the length for Unet
        shapex[1] = 64 - (shapex[1] % 64) + shapex[1]

    model_our = Unet1D_cond(    
        dim=64,
        num_classes=100,  # style condition embedding dim
        dim_mults=(1, 2, 4, 8),
        channels=shapex[2],
        context_using=True  # use style condition
    )

    diffusion = GaussianDiffusion1Dcond(
        model_our,
        seq_length=shapex[1],
        timesteps=100,  
        objective='pred_noise'
    )
    diffusion = diffusion.to(device)
    
    # Load global diffusion model if provided (for federated learning rounds after the first)
    if global_diffusion_model is not None:
        print(f"Loading global diffusion model for Client {client_id}")
        diffusion.load_state_dict(global_diffusion_model.state_dict())
    
    train_loader = source_loaders
    # ä¿®æ”¹ä¿å­˜è·¯å¾„ä»¥åŒºåˆ†ä¸åŒclient
    client_results_folder = os.path.join(args.results_folder, f'client_{client_id}')
    os.makedirs(client_results_folder, exist_ok=True)
    
    trainer = Trainer1D(
        diffusion,
        dataloader=train_loader,
        train_batch_size=shapex[0],
        train_lr=2e-4,
        train_num_steps=args.local_training_steps,  # ä½¿ç”¨æœ¬åœ°è®­ç»ƒæ­¥æ•°
        gradient_accumulate_every=2,
        ema_decay=0.995,
        amp=False,
        results_folder=client_results_folder
    )

    trainer.train(testuser_dict)
    print(f"Client {client_id} Diffusion Model è®­ç»ƒå®Œæˆ!")
    return diffusion

def aggregate_diffusion_models(client_models, args, round_num):
    """èšåˆå„ä¸ªclientçš„diffusion model"""
    print("=" * 50)
    print("å¼€å§‹èšåˆå„Clientçš„Diffusion Models")
    print("=" * 50)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªclientçš„æ¨¡å‹ä½œä¸ºåŸºç¡€
    global_model = client_models[0]
    global_state_dict = global_model.state_dict()
    
    # æ”¶é›†æ‰€æœ‰clientçš„å‚æ•°
    client_state_dicts = [model.state_dict() for model in client_models]
    
    # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œå¹³å‡
    for key in global_state_dict.keys():
        # è®¡ç®—æ‰€æœ‰clientè¯¥å‚æ•°çš„å¹³å‡å€¼
        param_sum = torch.zeros_like(global_state_dict[key])
        for client_state_dict in client_state_dicts:
            param_sum += client_state_dict[key]
        global_state_dict[key] = param_sum / len(client_models)
    
    # æ›´æ–°å…¨å±€æ¨¡å‹
    global_model.load_state_dict(global_state_dict)
    
    # ä¿å­˜èšåˆåçš„æ¨¡å‹
    global_results_folder = os.path.join(args.results_folder, 'global_diffusion_model')
    os.makedirs(global_results_folder, exist_ok=True)
    
    data_type = args.dataset
    target = args.target
    remain_rate = args.remain_rate
    SEED = args.seed
    global_model_name = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_global_round_{round_num}'
    
    # ä¿å­˜å…¨å±€æ¨¡å‹
    global_model_path = os.path.join(global_results_folder, global_model_name + '.pt')
    torch.save({
        'model': global_model.state_dict(),
        'step': args.local_training_steps,
        'aggregation_round': round_num
    }, global_model_path)
    
    print(f"å…¨å±€Diffusion Modelå·²ä¿å­˜åˆ°: {global_model_path}")
    print("Diffusion Modelèšåˆå®Œæˆ!")
    return global_model

def main():
    parser = argparse.ArgumentParser()

    ######################## Model parameters ########################
    home_dir = os.getcwd()
    
    # Common parameters
    parser.add_argument('--seed', default=1, type=int, help='seed value')
    parser.add_argument('--dataset', default='emg', type=str, help='Dataset of choice: pamap, uschad, dsads')
    parser.add_argument('--selected_dataset', default='emg', type=str, help='Dataset of choice: pamap, uschad, dsads')

    parser.add_argument('--remain_rate', default=0.2, type=float, help='Using training data ranging from 0.2 to 1.0')
    parser.add_argument('--remain_data_rate', default=0.2, type=float, help='Using training data ranging from 0.2 to 1.0')

    parser.add_argument('--target', default=0, type=int, help='Choose task id')
    parser.add_argument('--device', default='cuda', type=str, help='cpu or cuda')
    parser.add_argument('--batch_size', default=32, type=int, help='Training batch')
    
    # Style conditioner specific parameters
    parser.add_argument('--experiment_description', default='Exp1', type=str, help='Experiment Description')
    parser.add_argument('--run_description', default='run1', type=str, help='Experiment Description')
    parser.add_argument('--training_mode', default='self_supervised', type=str, help='Modes of choice: random_init, supervised, self_supervised, fine_tune, train_linear,rl')
    parser.add_argument('--logs_save_dir', default='./Style_conditioner/conditioner_pth/', type=str, help='saving directory')
    parser.add_argument('--home_path', default=home_dir, type=str, help='Project home directory')
    
    # Diffusion model specific parameters
    parser.add_argument('--results_folder', default='./Diffusion_model/dm_pth/', type=str, help='saving directory for diffusion model')
    
    # Federated learning parameters
    parser.add_argument('--local_training_steps', default=1, type=int, help='local training steps for each client')
    parser.add_argument('--aggregation_num', default=1, type=int, help='number of aggregation rounds')
    parser.add_argument('--num_clients', default=15, type=int, help='number of clients')
    parser.add_argument('--local_training_steps_style', default=1, type=int, help='local training steps for each client')
    parser.add_argument('--aggregation_num_style', default=1, type=int, help='number of aggregation rounds')

    args = parser.parse_args()

    # Create necessary directories
    os.makedirs(args.logs_save_dir, exist_ok=True)
    os.makedirs(args.results_folder, exist_ok=True)

    print(f"å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒæµç¨‹:")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"ç›®æ ‡: {args.target}")
    print(f"å‰©ä½™æ•°æ®æ¯”ä¾‹: {args.remain_rate}")
    print(f"ç§å­: {args.seed}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å®¢æˆ·ç«¯æ•°é‡: {args.num_clients}")
    print(f"æœ¬åœ°è®­ç»ƒæ­¥æ•°: {args.local_training_steps}")
    print(f"èšåˆè½®æ•°: {args.aggregation_num}")

    # ç”¨äºè®°å½•å„clientçš„testuserä¿¡æ¯
    client_testusers = []
    # å…¨å±€æ¨¡å‹å˜é‡
    global_style_model_path = None
    global_diffusion_model = None
    
    # Phase 1: Style Conditioner è”é‚¦å­¦ä¹ 
    print(f"\n{'='*80}")
    print(f"é˜¶æ®µ1: Style Conditioner è”é‚¦å­¦ä¹  ({args.aggregation_num_style} è½®)")
    print(f"{'='*80}")
    
    for round_num in range(args.aggregation_num_style):
        print(f"\n{'='*60}")
        print(f"Style Conditioner ç¬¬ {round_num + 1}/{args.aggregation_num_style} è½®")
        print(f"{'='*60}")
        
        # Step 1: å„ä¸ªclientè®­ç»ƒStyle Conditioner
        print(f"\nç¬¬{round_num + 1}è½®: å„Clientè®­ç»ƒStyle Conditioner")
        round_client_style_models = []
        round_client_temporal_models = []
        
        for client_id in range(1, args.num_clients + 1):
            testuser, style_model, temporal_model = train_style_conditioner_client(
                args, client_id, global_style_model_path
            )
            if round_num == 0:  # åªåœ¨ç¬¬ä¸€è½®è®°å½•testuser
                client_testusers.append(testuser)
            round_client_style_models.append(style_model)
            round_client_temporal_models.append(temporal_model)
        
        # Step 2: èšåˆå„clientçš„Style Conditioner
        print(f"\nç¬¬{round_num + 1}è½®: èšåˆå„Clientçš„Style Conditioners")
        global_style_model, global_temporal_model, global_style_model_path = aggregate_style_conditioners(
            round_client_style_models, round_client_temporal_models, args, round_num
        )
        
        print(f"Style Conditioner ç¬¬{round_num + 1}è½®å®Œæˆ!")
    
    print(f"\n{'='*80}")
    print(f"é˜¶æ®µ1å®Œæˆ: Style Conditioner è”é‚¦å­¦ä¹ å·²å®Œæˆ!")
    print(f"{'='*80}")
    
    # Phase 2: Diffusion Model è”é‚¦å­¦ä¹ 
    print(f"\n{'='*80}")
    print(f"é˜¶æ®µ2: Diffusion Model è”é‚¦å­¦ä¹  ({args.aggregation_num} è½®)")
    print(f"{'='*80}")
    if not client_testusers:  
        data_type = args.dataset
        target = args.target
        remain_rate = args.remain_rate
        SEED = args.seed
        for client_id in range(1, args.num_clients + 1):
            testuser = data_type+"_tar_"+str(target) +'_rm_'+str(remain_rate)+'seed_'+str(SEED)+f'_client{client_id}'
            client_testusers.append(testuser)

    for round_num in range(args.aggregation_num):
        print(f"\n{'='*60}")
        print(f"Diffusion Model ç¬¬ {round_num + 1}/{args.aggregation_num} è½®")
        print(f"{'='*60}")
        
        # Step 1: å„ä¸ªclientè®­ç»ƒDiffusion Model
        print(f"\nç¬¬{round_num + 1}è½®: å„Clientè®­ç»ƒDiffusion Model")
        round_client_diffusion_models = []
        
        for client_id in range(1, args.num_clients + 1):
            testuser = client_testusers[client_id - 1]  # ä½¿ç”¨å·²è®°å½•çš„testuser
            diffusion_model = train_diffusion_model_client(
                args, testuser, client_id, global_diffusion_model
            )
            round_client_diffusion_models.append(diffusion_model)
        
        # Step 2: èšåˆå„clientçš„Diffusion Model
        print(f"\nç¬¬{round_num + 1}è½®: èšåˆå„Clientçš„Diffusion Models")
        global_diffusion_model = aggregate_diffusion_models(
            round_client_diffusion_models, args, round_num
        )
        
        print(f"Diffusion Model ç¬¬{round_num + 1}è½®å®Œæˆ!")
    
    print("=" * 80)
    print("è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆ!")
    print(f"é˜¶æ®µ1: Style Conditioner è”é‚¦å­¦ä¹ ({args.aggregation_num}è½®) - å·²å®Œæˆ")
    print(f"é˜¶æ®µ2: Diffusion Model è”é‚¦å­¦ä¹ ({args.aggregation_num}è½®) - å·²å®Œæˆ")
    print(f"æœ€ç»ˆå…¨å±€Style Conditioneræ¨¡å‹å·²ä¿å­˜åˆ°: {global_style_model_path}")
    print(f"æœ€ç»ˆå…¨å±€Diffusion Modelå·²ä¿å­˜")
    print("=" * 80)

if __name__ == "__main__":
    main()
